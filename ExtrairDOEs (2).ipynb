{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffe0e0f3-4fd0-4996-93d9-56482ee00664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório DOEs já existe.\n",
      "PDF ja existe: do20240703p01.pdf\n",
      "PDF ja existe: do20240703p02.pdf\n",
      "PDF ja existe: do20240703p03.pdf\n",
      "PDF ja existe: do20240702p01.pdfPDF ja existe: do20240702p02.pdf\n",
      "\n",
      "Após baixado os DOEs, inicia a extração:\n",
      "\n",
      "Documento do20240702p01.pdf\n",
      "20240702 1\n",
      "Dados TXT salvos em: txt_extraidos/02-07-2024.txt\n",
      "Documento do20240702p02.pdf\n",
      "20240702 2\n",
      "Dados TXT salvos em: txt_extraidos/02-07-2024.txt\n",
      "Documento do20240703p01.pdf\n",
      "20240703 1\n",
      "Dados TXT salvos em: txt_extraidos/03-07-2024.txt\n",
      "Documento do20240703p02.pdf\n",
      "20240703 2\n",
      "Dados TXT salvos em: txt_extraidos/03-07-2024.txt\n",
      "Documento do20240703p03.pdf\n",
      "20240703 3\n",
      "Dados TXT salvos em: txt_extraidos/03-07-2024.txt\n"
     ]
    }
   ],
   "source": [
    "#Geração dos Arquivos TXT\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from keybert import KeyBERT\n",
    "from DownloadDOEs import Baixar_DOEs\n",
    "from Funções import extrair_orgaos_PDF, extrair_texto_entre_orgaos\n",
    "\n",
    "# Downloads necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Caminhos para pastas de texto\n",
    "txt_folder = 'txt_extraidos'\n",
    "\n",
    "# Cria uma pasta para os arquivos TXT se não existir\n",
    "if not os.path.exists(txt_folder):\n",
    "    os.makedirs(txt_folder)\n",
    "\n",
    "# Função para pré-processamento de texto\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Função para extração de palavras-chave usando KeyBERT\n",
    "def extract_keywords(text):\n",
    "    kw_model = KeyBERT()\n",
    "    try:\n",
    "        keywords = kw_model.extract_keywords(text, top_n=20)\n",
    "        return [kw[0] for kw in keywords]  # Retorna apenas a palavra-chave (str)\n",
    "    except IndexError:\n",
    "        return []  # Retorna uma lista vazia se não houver palavras-chave extraídas\n",
    "\n",
    "# Função principal para processar os documentos PDF e gerar arquivos TXT\n",
    "def gerar_arquivos_TXT():\n",
    "    # Cria uma pasta com o nome especificado e baixa os Diários Oficiais do Estado até número especificado dias atrás\n",
    "    Baixar_DOEs('DOEs', 2)\n",
    "\n",
    "    # Pasta que guarda quais DOEs já estão extraídos em .json\n",
    "    if not os.path.exists('DOEsExtraidos'):\n",
    "        os.makedirs('DOEsExtraidos')\n",
    "\n",
    "    print('Após baixado os DOEs, inicia a extração:\\n')\n",
    "\n",
    "    # Essa parte faz toda a extração de órgãos e conteúdo dos documentos baixados\n",
    "    pdfpasta = os.listdir('DOEs/')\n",
    "    pdfpasta.sort()\n",
    "\n",
    "    for P in pdfpasta:\n",
    "        if os.path.exists(os.path.join('DOEsExtraidos', P)):\n",
    "            print(P + ' Documento já utilizado')\n",
    "            os.remove(os.path.join('DOEs', P))\n",
    "        elif P.endswith('.pdf'):\n",
    "            print('Documento ' + P)\n",
    "            z = 'DOEs/' + P\n",
    "            bloco = z\n",
    "            bloco = bloco.replace('DOEs/do', '')\n",
    "            bloco = bloco.split('p')\n",
    "            caderno = bloco[1]\n",
    "            bloco = bloco[0]\n",
    "            caderno = caderno.replace('0', '')\n",
    "            caderno = caderno.replace('.', '')\n",
    "            caderno = int(caderno)\n",
    "            print(bloco, caderno)\n",
    "            datachar = bloco[6:8]\n",
    "            meschar = bloco[4:6]\n",
    "            anochar = bloco[0:4]\n",
    "\n",
    "            temp = f'{datachar}-{meschar}-{anochar}'  # Definir a data no formato correto\n",
    "\n",
    "            # Essa parte extrai os órgãos e conteúdo dos documentos baixados\n",
    "            listadocs = extrair_orgaos_PDF(z)\n",
    "            listacontextos = extrair_texto_entre_orgaos(listadocs)\n",
    "\n",
    "            listX = []\n",
    "\n",
    "            for c in range(0, len(listacontextos)):\n",
    "                if '(Continuação)' in listacontextos[c].nome or ' (Continuação)' in listacontextos[c].nome:\n",
    "                    listacontextos[c].nome = listacontextos[c].nome.replace(' (Continuação)', '')\n",
    "                    if isinstance(listacontextos[c].publicacao, list):\n",
    "                        for T in range(0, len(listacontextos[c].publicacao)):\n",
    "                            listX.append({\n",
    "                                'DATA': temp,\n",
    "                                'CADERNO': caderno,\n",
    "                                'PAGINA': listacontextos[c].publicacao[T].page1,\n",
    "                                'NOME': listacontextos[c].nome,\n",
    "                                'PUBLICACAO': 1 + T,\n",
    "                                'TITULO': None,\n",
    "                                'ASSUNTO': None,\n",
    "                                'TEXTO': listacontextos[c].publicacao[T].texto,\n",
    "                                'LISTANEGRITO': listacontextos[c].publicacao[T].negrito\n",
    "                            })\n",
    "                    else:\n",
    "                        listX.append({\n",
    "                            'DATA': temp,\n",
    "                            'CADERNO': caderno,\n",
    "                            'PAGINA': listacontextos[c].publicacao.page1,\n",
    "                            'NOME': listacontextos[c].nome,\n",
    "                            'PUBLICACAO': 1 + T,\n",
    "                            'TITULO': None,\n",
    "                            'ASSUNTO': None,\n",
    "                            'TEXTO': listacontextos[c].publicacao.texto,\n",
    "                            'LISTANEGRITO': listacontextos[c].publicacao.negrito\n",
    "                        })\n",
    "                else:\n",
    "                    if isinstance(listacontextos[c].publicacao, list):\n",
    "                        for T in range(0, len(listacontextos[c].publicacao)):\n",
    "                            listX.append({\n",
    "                                'DATA': temp,\n",
    "                                'CADERNO': caderno,\n",
    "                                'PAGINA': listacontextos[c].publicacao[T].page1,\n",
    "                                'NOME': listacontextos[c].nome,\n",
    "                                'PUBLICACAO': 1 + T,\n",
    "                                'TITULO': None,\n",
    "                                'ASSUNTO': None,\n",
    "                                'TEXTO': listacontextos[c].publicacao[T].texto,\n",
    "                                'LISTANEGRITO': listacontextos[c].publicacao[T].negrito\n",
    "                            })\n",
    "                    else:\n",
    "                        listX.append({\n",
    "                            'DATA': temp,\n",
    "                            'CADERNO': caderno,\n",
    "                            'PAGINA': listacontextos[c].publicacao.page1,\n",
    "                            'NOME': listacontextos[c].nome,\n",
    "                            'PUBLICACAO': 1 + T,\n",
    "                            'TITULO': None,\n",
    "                            'ASSUNTO': None,\n",
    "                            'TEXTO': listacontextos[c].publicacao.texto,\n",
    "                            'LISTANEGRITO': listacontextos[c].publicacao.negrito\n",
    "                        })\n",
    "\n",
    "            os.rename(os.path.join('DOEs', P), os.path.join('DOEsExtraidos', P))\n",
    "\n",
    "            if temp != '':\n",
    "                # Salvar listX em um arquivo TXT antes de prosseguir com a criação do JSON\n",
    "                txt_file_path = os.path.join(txt_folder, f'{temp}.txt')\n",
    "                with open(txt_file_path, 'w') as write_file:\n",
    "                    for item in listX:\n",
    "                        write_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "                print('Dados TXT salvos em:', txt_file_path)\n",
    "\n",
    "# Chamar a função para gerar os arquivos TXT\n",
    "gerar_arquivos_TXT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3fa15e-f45d-41c9-9bcd-4cf9d4a4d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/user/home/bruno.rogerio/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/user/home/bruno.rogerio/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.1-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.0.1)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m233.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.1-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m229.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/user/home/bruno.rogerio/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "\u001b[33m  WARNING: The script pypdfium2 is installed in '/user/home/bruno.rogerio/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pdfplumber is installed in '/user/home/bruno.rogerio/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.1 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac8b0450-d43b-4d51-9334-f324bab2ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /user/home/bruno.rogerio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em: json_extraidos/26-06-2024.json\n"
     ]
    }
   ],
   "source": [
    "#Gerar Arquivos JSON\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from keybert import KeyBERT\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Downloads necessários do NLTK para uso posterior\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Definição dos caminhos para as pastas de arquivos TXT e JSON\n",
    "txt_folder = 'txt_extraidos'\n",
    "json_folder = 'json_extraidos'\n",
    "\n",
    "# Cria a pasta para os arquivos JSON se ela não existir\n",
    "if not os.path.exists(json_folder):\n",
    "    os.makedirs(json_folder)\n",
    "\n",
    "# Função para selecionar as palavras-chave mais representativas de um cluster\n",
    "def get_representative_keywords(cluster_labels, keywords):\n",
    "    clusters = {}\n",
    "    # Agrupa palavras-chave por rótulos de cluster\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        if idx < len(keywords):\n",
    "            clusters[label].append(keywords[idx])\n",
    "    \n",
    "    # Encontra o cluster mais comum\n",
    "    most_common_cluster = max(clusters, key=lambda k: len(clusters[k]))\n",
    "    representative_keywords = clusters[most_common_cluster]\n",
    "    return representative_keywords\n",
    "\n",
    "# Função principal para processar cada arquivo TXT\n",
    "def processar_arquivo_txt(txt_file):\n",
    "    temp = os.path.basename(txt_file).replace('.txt', '')\n",
    "\n",
    "    # Carrega os dados do arquivo TXT\n",
    "    listX = []\n",
    "    with open(txt_file, 'r') as read_file:\n",
    "        for i, line in enumerate(read_file):\n",
    "            pub = json.loads(line.strip())\n",
    "            pub['TITULO'] = \"\"\n",
    "            pub['ASSUNTO'] = \"\"\n",
    "            listX.append(pub)\n",
    "\n",
    "    # Define o título e o assunto para cada publicação\n",
    "    for pub in listX:\n",
    "        texto = pub.get('TEXTO', '')\n",
    "        negrito = pub.get('LISTANEGRITO', [])\n",
    "\n",
    "        # Pré-processamento do texto\n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "        vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "        # Dividir o texto em trechos menores (sentenças)\n",
    "        trechos = nltk.sent_tokenize(texto)\n",
    "        \n",
    "        # Verifica se há pelo menos dois trechos\n",
    "        if len(trechos) < 2:\n",
    "            pub['TITULO'] = \"N/A\"\n",
    "            pub['ASSUNTO'] = \"N/A\"\n",
    "            continue  # Pula para a próxima publicação se não houver trechos suficientes\n",
    "\n",
    "        # Vetoriza todos os trechos juntos\n",
    "        X_text = vectorizer.fit_transform(trechos)\n",
    "\n",
    "        # Normalização Min-Max dos dados TF-IDF\n",
    "        scaler = MinMaxScaler()\n",
    "        X_normalized = scaler.fit_transform(X_text.toarray())\n",
    "\n",
    "        # Define os métodos de clusterização\n",
    "        kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
    "        dbscan = DBSCAN(eps=0.1, min_samples=2)\n",
    "        agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "\n",
    "        # Aplica clusterização nos vetores TF-IDF normalizados dos trechos\n",
    "        cluster_labels_kmeans = kmeans.fit_predict(X_normalized)\n",
    "        cluster_labels_dbscan = dbscan.fit_predict(X_normalized)\n",
    "        cluster_labels_hierarchical = agg_clustering.fit_predict(X_normalized)\n",
    "\n",
    "        # Combina rótulos usando abordagem de votação\n",
    "        combined_labels = np.vstack((cluster_labels_kmeans, cluster_labels_dbscan, cluster_labels_hierarchical))\n",
    "        final_labels, _ = mode(combined_labels, axis=0)\n",
    "\n",
    "        # Filtra trechos que estão em negrito\n",
    "        filtered_final_rep_keywords = [kw for kw in trechos if any(neg in kw for neg in negrito)]\n",
    "        \n",
    "        # Ordena as palavras-chave representativas pela importância usando KeyBERT\n",
    "        kw_model = KeyBERT()\n",
    "        importance_final = []\n",
    "        for kw in filtered_final_rep_keywords:\n",
    "            keywords = kw_model.extract_keywords(''.join(kw), top_n=1)\n",
    "            if keywords:\n",
    "                importance_final.append(keywords[0][1])\n",
    "            else:\n",
    "                importance_final.append(0.0)  # Valor padrão para casos vazios\n",
    "\n",
    "        sorted_importance_final = sorted(importance_final, reverse=True)\n",
    "        \n",
    "        sorted_final_keywords = [kw for _, kw in sorted(zip(sorted_importance_final, filtered_final_rep_keywords), reverse=True)]\n",
    "        \n",
    "        # Define o Título e o Assunto para o texto completo\n",
    "        if sorted_final_keywords:\n",
    "            pub['TITULO'] = ''.join(re.sub(r'\\([^)]*\\)', '', sorted_final_keywords[0]))  \n",
    "            if len(sorted_final_keywords) > 1:\n",
    "                pub['ASSUNTO'] = ''.join(re.sub(r'\\([^)]*\\)', '', sorted_final_keywords[1]))\n",
    "            else:\n",
    "                pub['ASSUNTO'] = \"N/A\"\n",
    "        else:\n",
    "            pub['TITULO'] = \"N/A\"\n",
    "            pub['ASSUNTO'] = \"N/A\"\n",
    "\n",
    "        # Remove o campo LISTANEGRITO antes de salvar\n",
    "        if 'LISTANEGRITO' in pub:\n",
    "            del pub['LISTANEGRITO']\n",
    "\n",
    "    # Salva os resultados em arquivos JSON\n",
    "    json_filename = os.path.join(json_folder, f\"{temp}.json\")\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(listX, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Resultados salvos em: {json_filename}\")\n",
    "\n",
    "# Chama a função principal para processar os arquivos TXT e gerar os arquivos JSON\n",
    "txt_files = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.endswith('.txt')]\n",
    "for txt_file in txt_files:\n",
    "    processar_arquivo_txt(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd8ae5-c578-44fa-ac46-775af7385072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
